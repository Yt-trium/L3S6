Introduction to Artificial Neural Networks & Deep Learning
===================================

Permettre a des programmes d'apprendre des choses tout seuls.

###1. Historique

40-50 : début de l'informatique moderne
1943 : McCulloch premiers neurones informatiques imaginés.
1950 : Création du test de Turing.
1952 : Programme d'échec de Samuel.
1957 : Perceptron : une machine qui reconnaît des images Rosenblatt.
1967 : Algorithme des plus proches voisins (calcul de distance entre différentes données)
avant 80 : Minsky soulève le problème suivant : le perceptron ne peut pas traiter des problèmes non linéraires. La recherche en IA est beaucoup moins financé
1986 : Algorithme de rétro-propagation 
1990 : ConvNet (réseau convolutifs) : découpage des données en couches.
1995 : SVM (Machine a vecteur de support) c'est a dire de la classification binaire.
2000 : Pas de vrai avancés, stagnation du milieu.
2012 : ImageNet dataset & GPU : création de réseaux convolutifs énorme qui atteigne les 16% d'erreur.
2012+ : Les taux d'erreurs sont proches des 5-6%.

- Comment fonctionne les réseaux de neurones ?
- Qu'est ce que le deep learning et comment l'associé au calcul parallèle

###2. Réseau de neurones

Un neurone est composé d'une routine qui prend des entrées binaire en les pondérant.
Le seuil est noté b (biais) x1*w1+x2*w2-b>=0 ?

Un réseau de neurones multi-couche (au moins 3 couches)
1. couche d'entrée
2. couche cachés
3. couche de sortie

La couche caché c'est un nom pour dire que ce n'est pas une entrée ou une sortie.

Il faut avoir un ensemble de test.

Retropropagation de gradient :
- Le but c'est de minimiser la fonction pour minimiser les erreurs.

#### Notations

Exemple avec un réseau :

x1, x2, x3 (les entrées)
les entrées sont entièrement connecté a la couche caché
y1, y2, y3 (les sorties)

En entraînement, on a un vecteur d'entrée et de sortie fixe.
(exemple y=(0.1,0.7,0.2) x=(0.5,0.8,0.1))

Les arcs sont pondérés -> Voir avec le dessin.

##### Stochastic Gradient Descente

Même principe que la descente de gradient normal, mais sans le faire sur toutes les couches (mini-batch)









